# Teacher-Student UNet for Self-Training Gland Segmentation

A comprehensive implementation of Teacher-Student learning for multi-task gland segmentation with self-training capabilities.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Key Features](#key-features)
- [Installation & Setup](#installation--setup)
- [Usage](#usage)
- [Configuration](#configuration)
- [Training Protocol](#training-protocol)
- [Evaluation](#evaluation)
- [File Structure](#file-structure)
- [Technical Details](#technical-details)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

The Teacher-Student UNet implements a self-training framework for gland segmentation that combines:

- **4-class Segmentation**: Background(0), Benign(1), Malignant(2), PDC(3)
- **Multi-task Learning**: Segmentation + patch/gland classification
- **Self-training**: Teacher provides pseudo-labels for consistency loss
- **Two-phase Training**: Warm-up (student only) â†’ Teacher-Student (dual loss)
- **Flexible Backbones**: Choose between BaselineUNet or nnUNet architectures

### Research Context
- **Target**: Gland segmentation in histopathology images
- **Datasets**: Warwick GlaS + OSU Makoto combined datasets
- **Method**: Exponential Moving Average (EMA) teacher updates
- **Evaluation**: Teacher-only protocol (best practice for self-training)

## ğŸ—ï¸ Architecture

### Core Components

```
Teacher-Student UNet
â”œâ”€â”€ Student Network (Trainable)
â”‚   â”œâ”€â”€ Configurable Backbone (BaselineUNet or nnUNet)
â”‚   â”œâ”€â”€ Segmentation head (4 classes)
â”‚   â”œâ”€â”€ Patch classification head
â”‚   â””â”€â”€ Gland classification head
â”œâ”€â”€ Teacher Network (EMA-only)
â”‚   â”œâ”€â”€ Same architecture as Student
â”‚   â”œâ”€â”€ No gradient updates
â”‚   â””â”€â”€ Updated via EMA: Î¸_t = Î±Â·Î¸_t + (1-Î±)Â·Î¸_s
â””â”€â”€ Loss Functions
    â”œâ”€â”€ Supervised Loss (Student vs Ground Truth)
    â”œâ”€â”€ Consistency Loss (Student vs Teacher)
    â””â”€â”€ Combined: L = Î±Â·L_sup + (1-Î±)Â·L_con
```

### Network Details

- **Backbone Options**:
  - **BaselineUNet**: U-Net with skip connections (~31M parameters per network)
  - **nnUNet**: State-of-the-art medical segmentation architecture (~21M parameters per network)
- **Input**: RGB images (3 channels)
- **Output**: Multi-task predictions (segmentation + classification)
- **Total Parameters**:
  - BaselineUNet backbone: ~62M (31M Ã— 2 networks)
  - nnUNet backbone: ~41M (21M Ã— 2 networks)
- **Teacher Updates**: EMA with decay rate 0.999

## âœ¨ Key Features

### Two-Phase Training Protocol

1. **Warm-up Phase** (Epochs 0-49)
   - Only student network trains with supervised loss
   - Teacher network not initialized
   - Focus on learning basic segmentation

2. **Teacher-Student Phase** (Epochs 50+)
   - Teacher initialized with student weights
   - Dual loss: supervised + consistency
   - Teacher updated via EMA after each batch
   - Cosine decay loss weighting

### Advanced Loss System

- **Supervised Loss**: Traditional multi-task loss (CrossEntropy + BCE)
- **Consistency Loss**: Student predictions vs Teacher pseudo-labels
  - Segmentation consistency (MSE/KL-div)
  - Patch classification consistency
  - Gland classification consistency
- **Alpha Scheduling**: Cosine decay from 1.0 â†’ 0.1 over training

### Self-Training Benefits

- **Pseudo-labeling**: Teacher provides high-quality pseudo-labels
- **Regularization**: Consistency loss acts as regularization
- **Stability**: EMA updates provide stable teacher weights
- **Performance**: Often outperforms single-network baselines

### ğŸ›ï¸ Backbone Architecture Selection

The Teacher-Student UNet supports two backbone architectures:

#### **BaselineUNet Backbone (Default)**
- **Architecture**: Classic U-Net with skip connections
- **Parameters**: ~31M per network (62M total)
- **Features**: Simple, interpretable, fast training
- **Best for**: Baseline comparisons, interpretable results
- **Bottleneck**: 1024 channels

#### **nnUNet Backbone (Advanced)**
- **Architecture**: State-of-the-art medical segmentation network
- **Parameters**: ~21M per network (41M total)
- **Features**: Deep supervision, optimized for medical images
- **Best for**: Maximum performance, advanced features
- **Bottleneck**: 512 channels
- **Special**: Multi-scale deep supervision outputs

**Configuration:**
```bash
# In run_nnunet_training.sh
TS_BACKBONE_TYPE="baseline_unet"  # Default: simple U-Net
TS_BACKBONE_TYPE="nnunet"         # Advanced: nnU-Net architecture
```

**Output Directory Naming:**
The backbone type is automatically included in experiment folder names for easy identification:

```bash
# Example output directories:
teacher_student_baseline_mag5x_20250924_130210    # BaselineUNet backbone
teacher_student_nnunet_mag5x_20250924_130210      # nnUNet backbone
teacher_student_nnunet_mixed_enhanced_20250924... # Enhanced training with nnUNet
```

## ğŸš€ Installation & Setup

### Prerequisites

```bash
# Required packages
torch >= 1.9.0
torchvision
numpy
matplotlib
opencv-python
scikit-learn
tqdm
```

### Environment Setup

```bash
# Clone repository
cd /users/PAS2942/hikmat179/Code/_MLCRC/GlandSegmentation/GlandSegModels/nnUNet

# Verify implementation
python -c "from src.models.teacher_student_unet import TeacherStudentUNet; print('âœ… Import successful')"
```

## ğŸ’» Usage

### Basic Training Command

```bash
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mixed \
  --output_base_dir /path/to/experiments \
  --experiment_name teacher_student_exp_001 \
  --num_epochs 200 \
  --batch_size 8 \
  --learning_rate 1e-4
```

### Teacher-Student Specific Options

```bash
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mixed \
  --output_base_dir /path/to/experiments \
  \
  # Teacher-Student Configuration
  --ema_decay 0.999 \
  --teacher_init_epoch 50 \
  --total_epochs 200 \
  --warmup_epochs 50 \
  \
  # Loss Configuration
  --consistency_loss_type mse \
  --consistency_temperature 1.0 \
  --min_alpha 0.1 \
  --max_alpha 1.0 \
  \
  # Training Configuration
  --num_epochs 200 \
  --batch_size 8 \
  --learning_rate 1e-4 \
  --weight_decay 1e-5

# Note: Backbone type is configured via run_nnunet_training.sh:
# TS_BACKBONE_TYPE="baseline_unet"  # or "nnunet"
```

### SLURM Job Submission

```bash
sbatch --job-name=teacher_student_training \
       --time=48:00:00 \
       --gres=gpu:1 \
       --mem=32G \
       --wrap="python main.py --architecture teacher_student_unet --dataset_key mixed --output_base_dir /fs/scratch/PAS2942/experiments"
```

## âš™ï¸ Configuration

### Model Configuration

```python
# Teacher-Student UNet Configuration
teacher_student_config = {
    "architecture": "teacher_student_unet",
    "input_channels": 3,
    "num_classes": 4,

    # Teacher-Student specific
    "backbone_type": "baseline_unet",  # "baseline_unet" or "nnunet"
    "ema_decay": 0.999,
    "teacher_init_epoch": 50,
    "teacher_init_val_loss": None,  # Optional validation loss threshold

    # BaselineUNet configuration (used when backbone_type="baseline_unet")
    "depth": 4,
    "initial_channels": 64,
    "activation": "relu",
    "normalization": "batch",
    "dropout": 0.1,
    "bilinear": True,
    "enable_hooks": True,

    # nnUNet configuration (used when backbone_type="nnunet")
    "deep_supervision": True,
    "architecture_type": "PlainConvUNet"  # or "ResidualEncoderUNet"
}
```

### Loss Configuration

```python
# Teacher-Student Loss Configuration
loss_config = {
    "total_epochs": 200,
    "warmup_epochs": 50,
    "min_alpha": 0.1,      # Maximum consistency weight
    "max_alpha": 1.0,      # Maximum supervised weight

    # Consistency loss settings
    "consistency_loss_config": {
        "segmentation_weight": 1.0,
        "patch_classification_weight": 0.5,
        "gland_classification_weight": 0.5,
        "temperature": 1.0,
        "loss_type": "mse"  # Options: "mse", "kl_div", "l1"
    },

    # Supervised loss settings (inherited from MultiTaskLoss)
    "supervised_loss_config": {
        "segmentation_weight": 1.0,
        "patch_classification_weight": 0.5,
        "gland_classification_weight": 0.5
    }
}
```

## ğŸ“š Training Protocol

### Phase 1: Warm-up (Epochs 0-49)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Student   â”‚    â”‚ Supervised   â”‚
â”‚   Network   â”œâ”€â”€â”€â”€â”¤     Loss     â”‚
â”‚ (trainable) â”‚    â”‚   (only)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Teacher: Not initialized
Loss: L = L_supervised
Alpha: 1.0 (pure supervised)
```

### Phase 2: Teacher-Student (Epochs 50+)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Student   â”‚    â”‚  Combined    â”‚
â”‚   Network   â”œâ”€â”€â”€â”€â”¤     Loss     â”‚
â”‚ (trainable) â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ L = Î±Â·L_sup  â”‚
                   â”‚ + (1-Î±)Â·L_conâ”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚              â”‚
â”‚   Teacher   â”‚    â”‚              â”‚
â”‚   Network   â”œâ”€â”€â”€â”€â”˜              â”‚
â”‚ (EMA-only)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Teacher: EMA updates after each batch
Loss: Combined supervised + consistency
Alpha: Cosine decay 1.0 â†’ 0.1
```

### Training Timeline

| Epoch Range | Phase | Alpha Range | Teacher Status | Primary Learning |
|-------------|-------|-------------|----------------|------------------|
| 0-49        | Warmup | 1.0 | Not initialized | Basic segmentation |
| 50-100      | TS Early | 1.0 â†’ 0.55 | Active, stabilizing | Supervised + light consistency |
| 100-150     | TS Mid | 0.55 â†’ 0.25 | Stable | Balanced learning |
| 150-200     | TS Late | 0.25 â†’ 0.1 | Mature | Heavy consistency focus |

## ğŸ“Š Evaluation

### Teacher-Only Evaluation Protocol

The teacher network is used for final evaluation (best practice for self-training):

```python
# Evaluation mode
model.eval()
outputs = model(images, mode="teacher_only")
predictions = outputs['teacher']

# Extract predictions
seg_preds = predictions['segmentation']
patch_preds = predictions['patch_classification']
gland_preds = predictions['gland_classification']
```

### ğŸ¨ Enhanced 6-Column Visualizations

Teacher-Student models generate comprehensive visualization reports during post-training evaluation:

#### Visualization Layout
1. **Original Image** - Unnormalized input image with ground truth classes and patch classification
2. **Ground Truth Mask** - Color-coded segmentation mask showing true tissue classes
3. **Teacher Pseudo-Mask** - Teacher network predictions with prediction probabilities for all 4 classes (e.g., "Bac: 0.95 | Ben: 0.78 | Mal: 0.12 | PDC: 0.03") and teacher patch classification
4. **Ground Truth Overlay** - Ground truth mask overlaid on original image with GT classes in title
5. **Teacher Overlay** - Teacher predictions overlaid on original image with teacher prediction probabilities and patch classification
6. **Student Overlay** - Student predictions overlaid on original image with student prediction probabilities and patch classification

#### Key Features
- **Prediction Probabilities**: All overlay columns display detailed class probabilities for all 4 classes (Background, Benign, Malignant, PDC)
- **Independent Networks**: Teacher and student predictions shown separately, highlighting differences
- **Comprehensive Information**: Both segmentation probabilities and patch classification predictions in column titles
- **Visual Comparison**: Easy comparison between ground truth, teacher predictions, and student predictions
- **Overlay Analysis**: Ground truth and both networks overlaid on original image for clear visual assessment

#### Example Column Titles
```
Column 3: Teacher Pseudo-Mask
          Bac: 0.95 | Ben: 0.78 | Mal: 0.12 | PDC: 0.03
          Patch: Background+Benign

Column 5: Teacher Overlay
          Seg: Bac: 0.95 | Ben: 0.78 | Mal: 0.12 | PDC: 0.03
          Patch: Background+Benign

Column 6: Student Overlay
          Seg: Bac: 0.92 | Ben: 0.81 | Mal: 0.19 | PDC: 0.05
          Patch: Background+Benign+Malignant
```

This enhanced visualization system enables detailed analysis of teacher-student learning dynamics and prediction consistency across both networks.

## ğŸ’¾ Model Checkpoints

### Automatic Checkpoint Saving

Teacher-Student UNet training automatically saves **six separate checkpoint files**:

| Checkpoint File | Description | Contains |
|----------------|-------------|----------|
| `best_model.pth` | Complete best model | Full MultiTaskWrapper with both networks |
| `latest_model.pth` | Complete latest model | Full MultiTaskWrapper with both networks |
| `best_student_model.pth` | Best student network only | Student UNet + training state |
| `best_teacher_model.pth` | Best teacher network only | Teacher UNet (EMA weights) |
| `latest_student_model.pth` | Latest student network | Student UNet + training state |
| `latest_teacher_model.pth` | Latest teacher network | Teacher UNet (EMA weights) |

### Loading Individual Networks

```python
from src.training.trainer import MultiTaskTrainer

# Load different checkpoint types
best_checkpoint = MultiTaskTrainer.load_teacher_student_checkpoint("/path/to/models/", "best")
teacher_checkpoint = MultiTaskTrainer.load_teacher_student_checkpoint("/path/to/models/", "teacher")
student_checkpoint = MultiTaskTrainer.load_teacher_student_checkpoint("/path/to/models/", "student")

# Access individual network weights
teacher_weights = teacher_checkpoint['teacher_state_dict']
student_weights = student_checkpoint['student_state_dict']
full_model_weights = best_checkpoint['model_state_dict']
```

### Checkpoint Contents

**Teacher-specific checkpoints** include:
- `teacher_state_dict`: Teacher network weights (EMA-updated)
- `teacher_initialized`: Teacher initialization status
- `ema_decay`: EMA decay rate used
- `epoch`, `metrics`, `config`: Training metadata

**Student-specific checkpoints** include:
- `student_state_dict`: Student network weights
- `optimizer_state_dict`: Optimizer state for resuming
- `scheduler_state_dict`: Learning rate scheduler state
- `teacher_initialized`: Teacher initialization status
- `ema_decay`: EMA decay rate used
- `epoch`, `metrics`, `config`, `train_history`: Training metadata

**Benefits:**
- **Flexibility**: Load only the network you need
- **Analysis**: Compare teacher vs student evolution
- **Inference**: Use teacher-only for final predictions
- **Resume Training**: Student checkpoint contains full training state
- **Research**: Analyze EMA weight progression

### Metrics Tracked

**Segmentation Metrics:**
- Dice Score (primary metric)
- IoU (Intersection over Union)
- Pixel Accuracy
- Class-wise performance

**Classification Metrics:**
- Accuracy
- Precision/Recall/F1
- AUC-ROC (for patch classification)

**Training Metrics:**
- Total Loss
- Supervised Loss
- Consistency Loss
- Alpha value (loss weighting)
- Teacher initialization status

## ğŸ“ File Structure

```
Teacher-Student UNet Implementation
â”œâ”€â”€ src/models/
â”‚   â”œâ”€â”€ teacher_student_unet.py      # Main architecture
â”‚   â”œâ”€â”€ teacher_student_loss.py      # Loss functions & scheduling
â”‚   â”œâ”€â”€ model_factory.py             # Updated with TS support
â”‚   â””â”€â”€ multi_task_wrapper.py        # Updated wrapper
â”œâ”€â”€ src/training/
â”‚   â””â”€â”€ teacher_student_trainer.py   # Specialized trainer
â”œâ”€â”€ main.py                          # Updated CLI with TS options
â””â”€â”€ Teacher_Student_UNet_README.md   # This file
```

### Key Files Description

**`teacher_student_unet.py`**
- TeacherStudentUNet class
- EMA update logic
- Teacher initialization
- Multi-mode forward pass

**`teacher_student_loss.py`**
- ConsistencyLoss implementation
- CosineDecayScheduler
- TeacherStudentLoss wrapper
- Multi-task consistency

**`teacher_student_trainer.py`**
- Two-phase training protocol
- EMA updates after each batch
- Dual model checkpointing
- Teacher-only evaluation

## ğŸ”§ Technical Details

### EMA Update Formula

```python
# After each training batch
for teacher_param, student_param in zip(teacher.parameters(), student.parameters()):
    teacher_param.data = ema_decay * teacher_param.data + (1 - ema_decay) * student_param.data
```

### Cosine Decay Schedule

```python
# Alpha calculation for epoch t
adjusted_epoch = current_epoch - warmup_epochs
adjusted_total = total_epochs - warmup_epochs
cosine_factor = 0.5 * (1 + cos(Ï€ * adjusted_epoch / adjusted_total))
alpha = min_alpha + (max_alpha - min_alpha) * cosine_factor
```

### Consistency Loss Types

1. **MSE Loss**: `L_con = MSE(student_logits, teacher_logits)`
2. **KL Divergence**: `L_con = KL(softmax(student/Ï„), softmax(teacher/Ï„)) * Ï„Â²`
3. **L1 Loss**: `L_con = L1(student_logits, teacher_logits)`

## ğŸ’¡ Examples

### Example 1: Standard Training

```bash
# Train Teacher-Student UNet on mixed dataset
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mixed \
  --output_base_dir /fs/scratch/PAS2942/teacher_student_experiments \
  --experiment_name standard_ts_training \
  --num_epochs 200 \
  --batch_size 8 \
  --learning_rate 1e-4 \
  --ema_decay 0.999 \
  --teacher_init_epoch 50
```

### Example 2: Custom Configuration

```bash
# Custom Teacher-Student configuration
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mag20x \
  --output_base_dir /fs/scratch/PAS2942/custom_ts_exp \
  --experiment_name custom_consistency_training \
  --num_epochs 300 \
  --batch_size 6 \
  --learning_rate 5e-5 \
  --ema_decay 0.995 \
  --teacher_init_epoch 75 \
  --consistency_loss_type kl_div \
  --consistency_temperature 3.0 \
  --min_alpha 0.05 \
  --max_alpha 0.95
```

### Example 3: nnUNet Backbone Training

```bash
# Train with nnU-Net backbone for advanced performance
# First, configure in run_nnunet_training.sh:
# TS_BACKBONE_TYPE="nnunet"

# Then run training:
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mixed \
  --output_base_dir /fs/scratch/PAS2942/nnunet_ts_exp \
  --experiment_name nnunet_backbone_training \
  --num_epochs 200 \
  --batch_size 6 \
  --learning_rate 1e-4 \
  --ema_decay 0.999 \
  --teacher_init_epoch 50
```

### Example 4: BaselineUNet vs nnUNet Comparison

```bash
# Configuration for BaselineUNet backbone
TS_BACKBONE_TYPE="baseline_unet"
EXPERIMENT_NAME="baseline_ts_training"

# Configuration for nnUNet backbone
TS_BACKBONE_TYPE="nnunet"
EXPERIMENT_NAME="nnunet_ts_training"

# Use same training parameters for fair comparison
# Expected results:
# - BaselineUNet: ~62M parameters, interpretable
# - nnUNet: ~41M parameters, better performance
```

### Example 5: Validation Loss-based Teacher Init

```bash
# Initialize teacher when validation loss drops below threshold
python main.py \
  --architecture teacher_student_unet \
  --dataset_key mixed \
  --output_base_dir /fs/scratch/PAS2942/adaptive_ts_exp \
  --experiment_name adaptive_teacher_init \
  --num_epochs 200 \
  --teacher_init_val_loss 0.5 \
  --ema_decay 0.999
```

## ğŸ› Troubleshooting

### Common Issues

**1. Teacher Not Initializing**
```bash
# Check teacher initialization criteria
--teacher_init_epoch 50      # Epoch-based (default)
--teacher_init_val_loss 0.5  # Or validation loss-based
```

**2. Memory Issues**
```bash
# Reduce batch size for dual networks
--batch_size 4  # Instead of 8
--mixed_precision  # Enable if available
```

**3. Slow Convergence**
```bash
# Adjust learning rate and EMA decay
--learning_rate 1e-4  # Try different rates
--ema_decay 0.99      # Faster teacher updates
```

**4. Consistency Loss Dominating**
```bash
# Adjust alpha range
--min_alpha 0.2       # Reduce consistency influence
--max_alpha 1.0       # Keep supervised learning strong
```

**5. Backbone Configuration Issues**
```bash
# nnUNet backbone missing dependencies
# Make sure environment variables are set:
export GLAND_DATASET_BASE="/path/to/dataset"
export NNUNET_PREPROCESSED="/path/to/preprocessed"
export NNUNET_RESULTS="/path/to/results"

# Check backbone type configuration
grep "TS_BACKBONE_TYPE" run_nnunet_training.sh
# Should show: TS_BACKBONE_TYPE="baseline_unet" or "nnunet"

# Verify backbone compatibility
python -c "
from src.models.teacher_student_unet import create_teacher_student_unet
model = create_teacher_student_unet(backbone_type='nnunet')
print('âœ… nnUNet backbone available')
"
```

**6. Parameter Count Differences**
```bash
# Expected parameter counts:
# BaselineUNet: ~62M total (31M per network)
# nnUNet: ~41M total (21M per network)

# Check actual parameters:
python -c "
from src.models.teacher_student_unet import create_teacher_student_unet
model = create_teacher_student_unet(backbone_type='baseline_unet')
print(f'BaselineUNet: {sum(p.numel() for p in model.parameters()):,} parameters')
model = create_teacher_student_unet(backbone_type='nnunet')
print(f'nnUNet: {sum(p.numel() for p in model.parameters()):,} parameters')
"
```

### Performance Tips

1. **Backbone Selection**:
   - **BaselineUNet**: Choose for interpretability, baseline comparisons, and stable training
   - **nnUNet**: Choose for maximum performance, parameter efficiency, and advanced features
   - **Memory**: nnUNet uses ~35% fewer parameters than BaselineUNet
   - **Speed**: BaselineUNet trains slightly faster, nnUNet may converge with fewer epochs

2. **Warmup Length**: Ensure sufficient warmup (â‰¥50 epochs) for student stability
3. **EMA Decay**: Higher values (0.999) for stable teachers, lower (0.99) for faster adaptation
4. **Consistency Loss**: Start with MSE, try KL-divergence for better calibration
5. **Batch Size**:
   - BaselineUNet: Can handle batch_size=8 on most GPUs
   - nnUNet: May allow batch_size=10+ due to lower memory usage
6. **Learning Rate**: May need reduction compared to single-network training

### Debugging Commands

```bash
# Check model creation
python -c "
from src.models.model_factory import ModelFactory
model = ModelFactory.create_segmentation_model('teacher_student_unet')
print(f'Model created: {type(model).__name__}')
"

# Test loss functions
python src/models/teacher_student_loss.py

# Test trainer
python src/training/teacher_student_trainer.py
```

## ğŸ“ˆ Expected Performance

### Typical Training Curves

**Loss Evolution:**
- Epochs 0-50: Only supervised loss decreases
- Epochs 50-100: Both losses active, alpha ~1.0â†’0.6
- Epochs 100-150: Consistency loss becomes dominant, alpha ~0.6â†’0.3
- Epochs 150-200: Heavy consistency focus, alpha ~0.3â†’0.1

**Performance Benchmarks:**
- **Dice Score**: Typically 2-5% improvement over baseline UNet
- **Convergence**: Faster initial convergence, better final performance
- **Stability**: More stable training due to consistency regularization

### Hyperparameter Sensitivity

| Parameter | Sensitivity | Recommended Range | Impact |
|-----------|-------------|-------------------|---------|
| EMA Decay | Medium | 0.995-0.999 | Teacher stability |
| Teacher Init Epoch | High | 30-70 | Learning dynamics |
| Min Alpha | Medium | 0.05-0.2 | Final consistency weight |
| Consistency Loss Type | Low | MSE/KL-div | Calibration quality |

## ğŸ“š References

1. **Teacher-Student Learning**: Tarvainen & Valpola, "Mean teachers are better role models" (NIPS 2017)
2. **Self-Training**: Xie et al., "Self-training with Noisy Student improves ImageNet classification" (CVPR 2020)
3. **Gland Segmentation**: Sirinukunwattana et al., "Gland segmentation in colon histology images" (TMI 2017)
4. **Multi-task Learning**: Zhang & Yang, "A survey on multi-task learning" (TKDE 2021)

---

**Generated by Claude Code for OSU CRC Research**
**Date**: 2025-09-22
**Version**: 1.0

For questions or issues, please refer to the troubleshooting section or contact the research team.